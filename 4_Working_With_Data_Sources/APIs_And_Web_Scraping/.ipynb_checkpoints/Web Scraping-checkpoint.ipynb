{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Introduction\n",
    "A lot of data aren't accessible through data sets or APIs. They may exist on the Internet as Web pages, though. One way to access the data without waiting for the provider to create an API is to use a technique called **Web scraping**.\n",
    "\n",
    "Web scraping allows us to load a Web page into Python and extract the information we want. We can then work with the data using standard analysis tools like `pandas` and `numpy`.\n",
    "\n",
    "Before we can do Web scraping, we need to understand the structure of the Web page we're working with, then find a way to extract parts of that structure in a sensible way.\n",
    "\n",
    "We'll use the `requests` library heavily as we learn about Web scraping. This library enables us to download a Web page. We'll also use the `beautifulsoup` library to extract the relevant parts of the Web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Web Page Structure\n",
    "Web pages use HyperText Markup Language (HTML). HTML isn't a programming language like Python. It's a markup language with its own syntax and rules. When a Web browser like Chrome or Firefox downloads a Web page, it reads the HTML to determine how to render it and display it to you.\n",
    "\n",
    "Here's the HTML for a very simple Web page:\n",
    "\n",
    "<img src=\"https://dq-content.s3.amazonaws.com/6ne0anS.png\">\n",
    "\n",
    "You can see what this page looks like [on our GitHub site](http://dataquestio.github.io/web-scraping-pages/simple.html).\n",
    "\n",
    "HTML consists of **tags**. We open a tag like this:\n",
    "\n",
    "<img src=\"https://dq-content.s3.amazonaws.com/tw5h1SV.png\">\n",
    "\n",
    "We close a tag like this:\n",
    "\n",
    "<img src=\"https://dq-content.s3.amazonaws.com/HP2OQvC.png\">\n",
    "\n",
    "Anything in between the opening and closing of a tag is the content of that tag. We can nest tags to create complex formatting rules. Here's an example:\n",
    "\n",
    "<img src=\"https://dq-content.s3.amazonaws.com/XGIR8GS.png\">\n",
    "\n",
    "The `b` tag bolds the content inside it, and the `p` tag creates a new paragraph. The HTML above will display as a bold paragraph because the `b` tag is inside the `p` tag. In other words, the `b` tag is nested within the `p` tag.\n",
    "\n",
    "HTML documents contain a few major sections. The `head` section contains information that's useful to the Web browser that's rendering the page; the user doesn't see it. The `body` section contains the bulk of the content the user interacts with on the page.\n",
    "\n",
    "Different tags have different purposes. For example, the `title` tag tells the Web browser what page title to display at the top of your tab. The `p` tag indicates that the content inside it is a single paragraph.\n",
    "\n",
    "We won't cover tags comprehensively here, but please read the [Mozilla Developer Network's (MDN) article on HTML basics](https://developer.mozilla.org/en-US/Learn/Getting_started_with_the_web/HTML_basics) if you need more of a grounding on this topic. Check out [MDN's guide to the HTML element](https://developer.mozilla.org/en-US/docs/Web/HTML/Element) for a list of all possible HTML tags. In order to do Web scraping effectively, you'll need a solid understanding of the various tags and how they work.\n",
    "\n",
    "In this exercise, you'll make a GET request to `http://dataquestio.github.io/web-scraping-pages/simple.html`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'<!DOCTYPE html>\\n<html>\\n    <head>\\n        <title>A simple example page</title>\\n    </head>\\n    <body>\\n        <p>Here is some simple content for this page.</p>\\n    </body>\\n</html>'\n"
     ]
    }
   ],
   "source": [
    "# Make a GET request to http://dataquestio.github.io/web-scraping-pages/simple.html, \n",
    "# and assign the result to the variable response.\n",
    "import requests\n",
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple.html\")\n",
    "\n",
    "# Use response.content to get the content of the response, and assign it to content.\n",
    "# Note how the content is the same as the HTML above.\n",
    "content = response.content\n",
    "print(content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Retrieving Elements From A Page\n",
    "Downloading the page is the easy part. Let's say that we want to get the text in the first paragraph. Now we need to parse the page and extract the information we want.\n",
    "\n",
    "We'll use the [BeautifulSoup]() library to parse the Web page with Python. This library allows us to extract tags from an HTML document.\n",
    "\n",
    "We can think of HTML documents as \"trees,\" and the nested tags as \"branches\" (similar to a family tree). BeautifulSoup works the same way.\n",
    "\n",
    "If we look at this page, for example, the root of the \"tree\" is the `html` tag:\n",
    "\n",
    "<img src=\"https://dq-content.s3.amazonaws.com/C7qmC17.png\">\n",
    "\n",
    "The `html` tag contains two \"branches,\" `head` and `body`. `head` contains one \"branch,\" `title`. `body` contains one branch, `p`. Drilling down through these multiple branches is one way to parse a Web page.\n",
    "\n",
    "To extract the text inside the `p` tag, we would first need to get the `body` element, then the `p` element, and then finally the text inside the `p` element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some simple content for this page.\n",
      "A simple example page\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Initialize the parser, and pass in the content we grabbed earlier.\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Get the body tag from the document.\n",
    "# Since we passed in the top level of the document to the parser, \n",
    "# we need to pick a branch off of the root.\n",
    "# With BeautifulSoup, we can access branches by using tag types as attributes.\n",
    "body = parser.body\n",
    "\n",
    "# Get the p tag from the body.\n",
    "p = body.p\n",
    "\n",
    "# Print the text inside the p tag.\n",
    "# Text is a property that gets the inside text of a tag.\n",
    "print(p.text)\n",
    "\n",
    "# Get the text inside the title tag, and asign the result to title_text\n",
    "title_text = parser.head.title.text\n",
    "print(title_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Using Find All\n",
    "While it's nice to use the tag type as a property, it's not always a very robust way to parse a document. It's usually better to be more explicit by using the `find_all` method. This method will find all occurrences of a tag in the current element, and return a list.\n",
    "\n",
    "If we only want the first occurrence of an item, we'll need to index the list to get it. Aside from this difference, it behaves the same way as passing in the tag type as an attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is some simple content for this page.\n",
      "A simple example page\n"
     ]
    }
   ],
   "source": [
    "# Get a list of all occurrences of the body tag in the element.\n",
    "body = parser.find_all(\"body\")\n",
    "\n",
    "# Get the paragraph tag.\n",
    "p = body[0].find_all(\"p\")\n",
    "# Get the text.\n",
    "print(p[0].text)\n",
    "\n",
    "# Apply the find_all method to get the text inside the title tag, \n",
    "# and assign the result to title_text.\n",
    "head = parser.find_all(\"head\")\n",
    "title = head[0].find_all(\"title\")\n",
    "title_text = title[0].text\n",
    "print(title_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Element IDs\n",
    "HTML allows elements to have **IDs**. Because they are unique, we can use an ID to refer to a specific element.\n",
    "\n",
    "Here's an example page:\n",
    "\n",
    "<img src=\"https://dq-content.s3.amazonaws.com/WBG4aCQ.png\">\n",
    "\n",
    "You can see the page [here](http://dataquestio.github.io/web-scraping-pages/simple_ids.html).\n",
    "\n",
    "HTML uses the `div` tag to create a divider that splits the page into logical units. We can think of a divider as a \"box\" that contains content. For example, different dividers hold a Web page's footer, sidebar, and horizontal menu.\n",
    "\n",
    "There are two paragraphs on the page; the first is nested inside a `div`. Luckily, the paragraphs have IDs. This means we can access them easily, even through they're nested.\n",
    "\n",
    "Let's use the `find_all` method to access those paragraphs, and pass in the additional `id` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                First paragraph.\n",
      "            \n",
      "\n",
      "\n",
      "                Second paragraph.\n",
      "            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the page content and set up a new parser.\n",
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple_ids.html\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Pass in the ID attribute to only get the element with that specific ID.\n",
    "first_paragraph = parser.find_all(\"p\", id=\"first\")[0]\n",
    "print(first_paragraph.text)\n",
    "\n",
    "# Get the text of the second paragraph (what's inside the second p tag),\n",
    "# and assign the result to second_paragraph_text.\n",
    "second_paragraph_text = parser.find_all(\"p\",id=\"second\")[0].text\n",
    "print(second_paragraph_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: Element Classes\n",
    "In HTML, elements can also have **classes**. Classes aren't globally unique. In other words, many different elements belong to the same class, usually because they share a common purpose or characteristic.\n",
    "\n",
    "For example, you may want to create three dividers to display three of your photographs. You can create a common look and feel for these dividers, such as a border and caption style.\n",
    "\n",
    "This is where classes come into play. You could create a class called \"gallery,\" define a style for it once using CSS (which we'll talk about soon), and then apply that class to all of the dividers you'll use to display photos. One element can even have multiple classes.\n",
    "\n",
    "<img src=\"https://dq-content.s3.amazonaws.com/T2TguLL.png\">\n",
    "\n",
    "Take a look at [this page](http://dataquestio.github.io/web-scraping-pages/simple_classes.html) to see how we've used classes to style paragraphs.\n",
    "\n",
    "We can use `find_all` to select elements by class. We'll just need to pass in the `class_` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                First paragraph.\n",
      "            \n",
      "\n",
      "                Second paragraph.\n",
      "            \n",
      "\n",
      "\n",
      "                First outer paragraph.\n",
      "            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the website that contains classes.\n",
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/simple_classes.html\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Get the first inner paragraph.\n",
    "# Find all the paragraph tags with the class inner-text.\n",
    "# Then, take the first element in that list.\n",
    "first_inner_paragraph = parser.find_all(\"p\", class_=\"inner-text\")[0]\n",
    "print(first_inner_paragraph.text)\n",
    "\n",
    "# Get the text in the second inner paragraph, \n",
    "# and assign the result to second_inner_paragraph_text.\n",
    "second_inner_paragraph_text = parser.find_all(\"p\", class_=\"inner-text\")[1].text\n",
    "print(second_inner_paragraph_text)\n",
    "\n",
    "# Get the text of the first outer paragraph, \n",
    "# and assign the result to first_outer_paragraph_text.\n",
    "first_outer_paragraph_text = parser.find_all(\"p\", class_=\"outer-text\")[0].text\n",
    "print(first_outer_paragraph_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: CSS Selectors\n",
    "\n",
    "**Cascading Style Sheets**, or **CSS**, is a language for adding styles to HTML pages. You may have noticed that our simple HTML pages from the past few screens didn't have any styling; all of the paragraphs had black text and the same font size. Most Web pages use CSS to display a lot more than basic black text.\n",
    "\n",
    "CSS uses **selectors** to add styles to the elements and classes of elements you specify. You can use selectors to add background colors, text colors, borders, padding, and many other style choices to the elements on HTML pages.\n",
    "\n",
    "An in-depth lesson on CSS is outside the scope of this mission. If you'd like to learn more about it on your own, [MDN's guide](https://developer.mozilla.org/en-US/docs/Web/Guide/CSS/Getting_started) is a great place to start.\n",
    "\n",
    "What we do need to know is how **CSS selectors** work.\n",
    "\n",
    "This CSS will make all of the text inside all paragraphs red:\n",
    "\n",
    "``p{\n",
    "    color: red\n",
    " }``\n",
    " \n",
    "You can see what this style looks like on our [GitHub site](http://dataquestio.github.io/web-scraping-pages/simple_red.html).\n",
    "\n",
    "This CSS will change the text color to red for any paragraphs that have the class `inner-text`. We select classes with the period or dot symbol (`.`):\n",
    "\n",
    "``p.inner-text{\n",
    "    color: red\n",
    " }``\n",
    " \n",
    " You can see what the result looks like [here](http://dataquestio.github.io/web-scraping-pages/simple_inner_red.html).\n",
    "\n",
    "This CSS will change the text color to red for any paragraphs that have the ID `first`. We select IDs with the pound or hash symbol (`#`):\n",
    " \n",
    " ``p#first{\n",
    "    color: red\n",
    " }``\n",
    " \n",
    " Take a look at the results [on our GitHub site](http://dataquestio.github.io/web-scraping-pages/simple_ids_red.html).\n",
    "\n",
    "You can also style IDs and classes without using any specific tags. For example, this CSS will make the element with the ID `first` red (not just paragraphs):\n",
    "\n",
    "``#first{\n",
    "    color: red\n",
    " }``\n",
    " \n",
    "This CSS will make any element with the class `inner-text` red:\n",
    "\n",
    "``.inner-text{\n",
    "    color: red\n",
    " }``\n",
    " \n",
    " In the examples above, we used CSS selectors to select one or more elements, then apply styles to only those elements. CSS selectors are very powerful and flexible.\n",
    "\n",
    "Perhaps not surprisingly, we also use CSS selectors to select elements when we do Web scraping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8: Using CSS Selectors\n",
    "We can use BeautifulSoup's `.select` method to work with CSS selectors. Here's the HTML we'll be working with on this screen:\n",
    "\n",
    "<img src=\"https://dq-content.s3.amazonaws.com/uOaCMeY.png\">\n",
    "\n",
    "You may have noticed that the same element can have both an ID and a class. We can also assign multiple classes to a single element; we just separate the classes with a space.\n",
    "\n",
    "Take a look at the [Web page](http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html) that corresponds to the HTML above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                First paragraph.\n",
      "            \n",
      "\n",
      "\n",
      "                First outer paragraph.\n",
      "            \n",
      "\n",
      "\n",
      "\n",
      "                First outer paragraph.\n",
      "            \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the website that contains classes and IDs.\n",
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/ids_and_classes.html\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Select all of the elements that have the first-item class.\n",
    "first_items = parser.select(\".first-item\")\n",
    "\n",
    "# Print the text of the first paragraph (the first element with the first-item class).\n",
    "print(first_items[0].text)\n",
    "\n",
    "# Select all of the elements that have the class outer-text.\n",
    "# Assign the text of the first paragraph that has the class outer-text \n",
    "# to first_outer_text.\n",
    "first_outer_text = parser.select(\".outer-text\")[0].text\n",
    "print(first_outer_text)\n",
    "\n",
    "# Select all of the elements that have the ID second.\n",
    "# Assign the text of the first paragraph that has the ID second \n",
    "# to the variable second_text.\n",
    "second_text = parser.select(\"#second\")[0].text\n",
    "print(second_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9: Nesting CSS Selectors\n",
    "\n",
    "We can nest CSS selectors similar to the way HTML nests tags. For example, we could use selectors to find all of the paragraphs inside the `body` tag. Nesting is a very powerful technique that enables us to use CSS to do complex Web scraping tasks.\n",
    "\n",
    "This selector will target any paragraph inside a div tag:\n",
    "\n",
    "``div p``\n",
    "\n",
    "This selector will target any item inside a `div` tag that has the class `first-item`:\n",
    "\n",
    "``div .first-item``\n",
    "\n",
    "This one is even more specific. It selects any item that's inside a `div` tag inside a `body` tag, but only if it also has the ID `first`:\n",
    "\n",
    "``body div #first``\n",
    "\n",
    "This selector zeroes in on any items with the ID `first` that are inside any items with the class `first-item`:\n",
    "\n",
    "``.first-item #first``\n",
    "\n",
    "As you can see, we can nest CSS selectors in infinite ways. This allows us to extract data from websites with complex layouts. You can test selectors by using the `.select` method as you write them. Because it's easy to write a selector that doesn't work the way you expect, we highly recommend doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10: Using Nested CSS Selectors\n",
    "Now that we know about nested CSS selectors, let's try them out. We can use them with the same `.select` method we used for our CSS selectors.\n",
    "\n",
    "We'll be practicing on this HTML:\n",
    "\n",
    "<img src=\"https://dq-content.s3.amazonaws.com/H34hK8I.png\">\n",
    "\n",
    "It's an excerpt from a box score of the [2014 Super Bowl](https://en.wikipedia.org/wiki/Super_Bowl_XLVIII), a [National Football League](https://en.wikipedia.org/wiki/National_Football_League) (NFL) game in which the New England Patriots played the Seattle Seahawks. The box score contains information on how many yards each team gained, how many turnovers each team had, and so on. Check out the [Web page](http://dataquestio.github.io/web-scraping-pages/2014_super_bowl.html) this HTML renders.\n",
    "\n",
    "The page renders as a table with column and row names. The first column is for the Seattle Seahawks, and the second column is for the New England Patriots. Each row represents a different statistic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "72\n",
      "396\n"
     ]
    }
   ],
   "source": [
    "# Get the Superbowl box score data.\n",
    "response = requests.get(\"http://dataquestio.github.io/web-scraping-pages/2014_super_bowl.html\")\n",
    "content = response.content\n",
    "parser = BeautifulSoup(content, 'html.parser')\n",
    "\n",
    "# Find the number of turnovers the Seahawks committed.\n",
    "turnovers = parser.select(\"#turnovers\")[0]\n",
    "seahawks_turnovers = turnovers.select(\"td\")[1]\n",
    "seahawks_turnovers_count = seahawks_turnovers.text\n",
    "print(seahawks_turnovers_count)\n",
    "\n",
    "# Find the Total Plays for the New England Patriots,\n",
    "# and assign the result to patriots_total_plays_count.\n",
    "total_plays = parser.select(\"#total-plays\")[0]\n",
    "patriots_total_plays_count = total_plays.select(\"td\")[2].text\n",
    "print(patriots_total_plays_count)\n",
    "\n",
    "# Find the Total Yards for the Seahawks, \n",
    "# and assign the result to seahawks_total_yards_count.\n",
    "total_yards = parser.select(\"#total-yards\")[0]\n",
    "seahawks_total_yards_count = total_yards.select(\"td\")[1].text\n",
    "print(seahawks_total_yards_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11: Beyond The Basics\n",
    "\n",
    "We've covered the basics of HTML and how to select elements, which are key foundational blocks.\n",
    "\n",
    "You may be wondering why Web scraping is useful, given that in most of our examples, we could easily have found the answer by looking at the page. The real power of Web scraping lies in getting information from a large amount of pages very quickly.\n",
    "\n",
    "Let's say we wanted to find the total number of yards each NFL team gained in every single NFL game over an entire season. We could do this manually, but it would take days of boring drudgery. We could write a script to automate this in a couple of hours instead, and have a lot more fun doing it."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
