{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For my own notes, this will not run correctly because some `OAuth` preprocessing steps have been omitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1: Reddit\n",
    "\n",
    "Over the past few missions, we learned how to make API requests, authenticate with an API server, and parse responses. In this challenge, you'll pull these concepts together to explore trending posts and comments on `reddit`.\n",
    "\n",
    "Reddit is a community-driven link-sharing site. Users submit links to articles, photos, and other content. Other users upvote the submissions they like, and downvote the ones they dislike. Users can comment on submissions, and even upvote or downvote other people's comments.\n",
    "\n",
    "Reddit consists of many smaller communities called subreddits where more focused communities can discuss niche posts. For example, `/r/pytho`n is a Python-focused community, and `/r/sanfrancisco` is for discussing issues relating to the city of `San Francisco, CA`. The posts you submit to a subreddit will appear on the group's front page if enough users upvote them. Very popular subreddit posts may appear on reddit's home page.\n",
    "\n",
    "Posts only stay on the main reddit and subreddit pages for a limited time. You can search for older posts, but it can be hard to find what you're looking for.\n",
    "\n",
    "In this challenge, you'll practice:\n",
    "\n",
    "- Retrieving a list of trending posts on a particular subreddit\n",
    "- Exploring the comments on a single article\n",
    "- Posting our own comment on an article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2: Authenticating With The API\n",
    "\n",
    "The reddit API requires authentication. You authenticated with a token in a previous mission, but in this challenge, you'll use [OAuth](https://en.wikipedia.org/wiki/OAuth). OAuth can be fairly complex, but we've done some of the hard work already. You'll be using an authentication token, `13426216-4U1ckno9J5AiK72VRbpEeBaMSKk`, to authenticate in much the same way we did earlier, except that the header will look like this:\n",
    "\n",
    "\n",
    "``{\"Authorization\": \"bearer 13426216-4U1ckno9J5AiK72VRbpEeBaMSKk\"}``\n",
    "\n",
    "\n",
    "Note that we'll need to use the string `bearer` instead of the string `token` we used in the previous mission. That's because we're using OAuth this time. While we won't discuss OAuth right now, you can read about it on Wikipedia and through [this blog post](https://blog.varonis.com/introduction-to-oauth/).\n",
    "\n",
    "We'll also need to add a User-Agent header, which will identify us as Dataquest to the API:\n",
    "\n",
    "\n",
    "``{\"Authorization\": \"bearer 13426216-4U1ckno9J5AiK72VRbpEeBaMSKk\", \"User-Agent\": \"Dataquest/1.0\"}``\n",
    "\n",
    "\n",
    "We've imported `requests` for you already, so please avoid doing it again in this mission. Importing `requests` will overwrite some of the custom API logic we've developed for answer checking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Retrieve the /r/python subreddit's top posts for the past day.\n",
    "# Make a GET request to https://oauth.reddit.com/r/python/top using the get method of the requests library. See the documentation for the /r/python/top endpoint if you need help.\n",
    "# Pass in the header information we showed you earlier in this section.\n",
    "# To retrieve only the top posts for the past day, pass in a query parameter t (for \"time\") and set its value to the string day.\n",
    "headers = {\"Authorization\": \"bearer 13426216-4U1ckno9J5AiK72VRbpEeBaMSKk\", \"User-Agent\": \"Dataquest/1.0\"}\n",
    "params = {\"t\":\"day\"}\n",
    "response = requests.get(\"https://oauth.reddit.com/r/python/top\", headers=headers,params=params)\n",
    "\n",
    "#Use the json method on the response to get the JSON response data.\n",
    "#Assign the JSON response data to the variable python_top.\n",
    "python_top = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Getting The Most Upvoted Post\n",
    "\n",
    "The variable `python_top` is a dictionary containing information about all of the individual posts that users submitted during the past day. However, the actual list of posts is buried inside a dictionary key, and you'll need to explore the dictionary to retrieve it. You can read more about `python_top`'s format [here](https://www.reddit.com/dev/api#listings).\n",
    "\n",
    "There's a dictionary for each individual post that looks like this:\n",
    "\n",
    "\n",
    "``{'data': {'approved_by': None,\n",
    "     'archived': False,\n",
    "     'author': 'ingvij',\n",
    "     ...\n",
    "     'ups': 43,\n",
    "     'url': 'http://hkupty.github.io/2016/Functional-Programming-Concepts-Idioms-and-Philosophy/',\n",
    "     'user_reports': [],\n",
    "     'visited': False},\n",
    "     'kind': 't3'}``\n",
    "     \n",
    "     \n",
    "We've truncated the representation, but you can see the `ups` field, which contains the number of people who upvoted the post. The `id` field holds reddit's unique ID for the post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Explore the python_top dictionary.\n",
    "#Extract the list containing all of the posts, and assign it to python_top_articles.\n",
    "python_top_articles = python_top[\"data\"][\"children\"]\n",
    "\n",
    "#Find the post with the most upvotes.\n",
    "#Assign the ID for the post with the most upvotes to most_upvoted.\n",
    "most_upvoted = \"\"\n",
    "most_upvotes = 0\n",
    "for article in python_top_articles:\n",
    "    ar = article[\"data\"]\n",
    "    if ar[\"ups\"] >= most_upvotes:\n",
    "        most_upvoted = ar[\"id\"]\n",
    "        most_upvotes = ar[\"ups\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Getting Post Comments\n",
    "\n",
    "Now that you have the ID for the most upvoted post, you can retrieve the comments on it using the [/r/{subreddit}/comments/{article}](https://www.reddit.com/dev/api#GET_comments_{article}) endpoint. You'll need to replace the items that have brackets around them with the appropriate values: `{subreddit}` - The name of the subreddit the post appears in (omit the leading /r, because it already exists). Use `python` for the `python` subreddit, for example. `{article}` - The ID for the post whose comments we want to retrieve. It should look like this: `4b7w9u`.\n",
    "\n",
    "You'll need to include the API's base URL, `https://oauth.reddit.com/`, before this endpoint to generate the full URL for the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get all of the comments on the /r/python subreddit's top post from the past day.\n",
    "# Generate the full URL to query, using the subreddit name and post ID.\n",
    "# Make a GET request to the URL.\n",
    "# Get the response data using the response's json method.\n",
    "# Assign the response data to the variable comments.\n",
    "\n",
    "headers = {\"Authorization\": \"bearer 13426216-4U1ckno9J5AiK72VRbpEeBaMSKk\", \"User-Agent\": \"Dataquest/1.0\"}\n",
    "response = requests.get(\"https://oauth.reddit.com/r/python/comments/4b7w9u\", headers=headers)\n",
    "comments = response.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Getting The Most Upvoted Comment\n",
    "Querying the comments endpoint at [/r/{subreddit}/comments/{article}](https://www.reddit.com/dev/api#GET_comments_{article}) returns a list. The first item in the list contains information about the post, and the second item contains information about the comments.\n",
    "\n",
    "Reddit users can nest comments. That is, they can comment on comments. Here's an [example](https://www.reddit.com/r/programming/comments/4b7uht/markov_chains_explained_visually/).\n",
    "\n",
    "This means that comments have one more key than posts do. The additional key, `replies`, contains the nested comments. You can read more about that structure in [reddit's API documentation](https://www.reddit.com/dev/api#listings). Here's an example of a single comment that has nested comments:\n",
    "\n",
    "``{'data': {'approved_by': None,\n",
    "      'archived': False,\n",
    "      'author': 'larsga',\n",
    "      ...\n",
    "      'replies': {'data': {'after': None,\n",
    "        'before': None,\n",
    "        'children': [{'data': {'approved_by': None,\n",
    "           'archived': False,\n",
    "           'author': 'Deto',\n",
    "           ...\n",
    "           },\n",
    "          ...\n",
    "          ]\n",
    "          }\n",
    "          ...\n",
    "          'url': 'https://www.reddit.com/r/Python/comments/4b6bew/using_pilpillow_with_mozjpeg/',\n",
    "         'user_reports': [],\n",
    "         'visited': False\n",
    "         }``\n",
    "         \n",
    "It's easier to focus on top-level comments only, and ignore the nested `replies`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Find the most upvoted top-level comment in comments.\n",
    "# Extract the comments list from the comments variable, and assign it to comments_list.\n",
    "# Assign the ID for the comment with the most upvotes to most_upvoted_comment.\n",
    "\n",
    "\n",
    "comments_list = comments[1][\"data\"][\"children\"]\n",
    "most_upvoted_comment = \"\"\n",
    "most_upvotes_comment = 0\n",
    "for comment in comments_list:\n",
    "    co = comment[\"data\"]\n",
    "    if co[\"ups\"] >= most_upvotes_comment:\n",
    "        most_upvoted_comment = co[\"id\"]\n",
    "        most_upvotes_comment = co[\"ups\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: Upvoting A Comment\n",
    "\n",
    "You can upvote a comment with the [/api/vote](https://www.reddit.com/dev/api#POST_api_vote) endpoint. You'll need to pass in the following parameters:\n",
    "\n",
    "- `dir` - Vote direction: `1`, `0`, or `-1`. `1` is an upvote, and `-1` is a downvote.\n",
    "- `id` - The ID for the post or comment to upvote."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make a POST request to the /api/vote endpoint to upvote the most upvoted comment from the last screen.\n",
    "# Assign the status code for the response to the variable status.\n",
    "\n",
    "\n",
    "payload = {\"dir\": 1, \"id\": \"d16y4ry\"}\n",
    "headers = {\"Authorization\": \"bearer 13426216-4U1ckno9J5AiK72VRbpEeBaMSKk\", \"User-Agent\": \"Dataquest/1.0\"}\n",
    "response = requests.post(\"https://oauth.reddit.com/api/vote\", json=payload, headers=headers)\n",
    "status = response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: Next Steps\n",
    "In this challenge, you authenticated with an API, then retrieved and parsed responses. In the next mission, we'll dive into Web scraping."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
