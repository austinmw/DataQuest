{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "### Learn about cross-validation and ROC to better test classification models.\n",
    "\n",
    "#### Contents:\n",
    "\n",
    "- Holdout validation\n",
    "- Accuracy\n",
    "- Sensitivity (TPR)\n",
    "- Specificity (TNR)\n",
    "- Fall-out (FPR)\n",
    "- receiver operating characteristic (ROC) curve\n",
    "- ROC area under curve (AUC)\n",
    "\n",
    "## 1: Introduction To Validation\n",
    "\n",
    "So far, we've been evaluating accuracy of trained models on the data the model was trained on. While this is an essential first step, this doesn't tell us much about how well the model does on data it's never seen before. In machine learning, we want to use training data, which is historical and contains the labelled outcomes for each observation, to build a classifier that will return predicted labels for new, unlabelled data. If we only evaluate a classifier's effectiveness on the data it was trained on, we can run into **overfitting**, where the classifier only performs well on the training but doesn't generalize to future data.\n",
    "\n",
    "To test a classifier's [generalizability](https://en.wikipedia.org/wiki/Generalizability_theory), or its ability to provide accurate predictions on data it wasn't trained on, we use cross-validation techniques. Cross-validation involves splitting historical data into:\n",
    "\n",
    "- **a training set** -- which we use to train the classifer,\n",
    "- **a test set** -- which we use to evaluate the classifier's effectiveness using various measures.\n",
    "\n",
    "Cross-validation is an important step that should be utilized after training any kind of machine learning model. In this mission, we'll focus on using cross-validation for evaluating a binary classification model. We'll continue to work with the dataset on graduate school admissions, which contains data on 644 applications with the following columns:\n",
    "\n",
    "- `gre` - applicant's store on the Graduate Record Exam, a generalized test for prospective graduate students.\n",
    "    - Score ranges from 200 to 800.\n",
    "- `gpa` - college grade point average.\n",
    "    - Continuous between 0.0 and 4.0.\n",
    "- `admit` - binary value\n",
    "    - Binary value, 0 or 1, where 1 means the applicant was admitted to the program and 0 means the applicant was rejected.\n",
    "\n",
    "In the following code cell, we import the libraries we need, read in the admissions Dataframe, rename the admit column to actual_label, and drop the admit column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpa</th>\n",
       "      <th>gre</th>\n",
       "      <th>actual_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.177277</td>\n",
       "      <td>594.102992</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3.412655</td>\n",
       "      <td>631.528607</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.728097</td>\n",
       "      <td>553.714399</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.093559</td>\n",
       "      <td>551.089985</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.141923</td>\n",
       "      <td>537.184894</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        gpa         gre  actual_label\n",
       "0  3.177277  594.102992             0\n",
       "1  3.412655  631.528607             0\n",
       "2  2.728097  553.714399             0\n",
       "3  3.093559  551.089985             0\n",
       "4  3.141923  537.184894             0"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "admissions = pd.read_csv(\"data/admissions.csv\")\n",
    "admissions[\"actual_label\"] = admissions[\"admit\"]\n",
    "admissions = admissions.drop(\"admit\", axis=1)\n",
    "\n",
    "admissions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Holdout Validation\n",
    "\n",
    "There are a few different types of cross-validation techniques we can use to evaluate a classifier's effectiveness. The simplest technique is called **holdout validation**, which involves:\n",
    "\n",
    "- randomly splitting our dataset into a training data and a test set,\n",
    "- fitting the model using the training set,\n",
    "- making predictions on the test set.\n",
    "\n",
    "We'll randomly select 80% of the observations in the admissions Dataframe as the training set and the remaining 20% as the test set. This ratio isn't set in stone, and you'll see many people using a 75%-25% split instead.\n",
    "\n",
    "We'll explore more advanced cross-validation techniques in later missions and will focus on holdout validation, the simplest kind of validation, in this mission. To split the data randomly into a training and a test set, we'll:\n",
    "\n",
    "- use the numpy.random.permutation function to return a list containing index values in random order,\n",
    "- return a new Dataframe in that list's order,\n",
    "- select the first 80% of the rows as the training set,\n",
    "- select the last 20% of the rows as the test set.\n",
    "\n",
    "#### Instructions:\n",
    "- Use the NumPy random.permutation function to randomize the index for the admissions Dataframe.\n",
    "- Use the loc[] method on the admissions Dataframe to return a new Dataframe in the randomized order. Assign this Dataframe to shuffled_admissions.\n",
    "- Select rows 0 to 514 (including row 514) from shuffled_admissions and assign to train.\n",
    "- Select the remaining rows and assign to test.\n",
    "- Finally, display the first 5 rows in shuffled_admissions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gpa</th>\n",
       "      <th>gre</th>\n",
       "      <th>actual_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>3.414036</td>\n",
       "      <td>577.665610</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>173</th>\n",
       "      <td>2.932147</td>\n",
       "      <td>564.798764</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>2.674040</td>\n",
       "      <td>599.895858</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>167</th>\n",
       "      <td>2.923581</td>\n",
       "      <td>622.524665</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>3.414376</td>\n",
       "      <td>704.934217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          gpa         gre  actual_label\n",
       "260  3.414036  577.665610             0\n",
       "173  2.932147  564.798764             0\n",
       "256  2.674040  599.895858             0\n",
       "167  2.923581  622.524665             0\n",
       "400  3.414376  704.934217             1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(8)\n",
    "\n",
    "shuffled_admissions = admissions.loc[np.random.permutation(admissions.index)]\n",
    "train = shuffled_admissions[:515]\n",
    "test = shuffled_admissions[515:]\n",
    "\n",
    "shuffled_admissions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Accuracy\n",
    "\n",
    "Now that we've split up the dataset into a training and a test set, we can:\n",
    "\n",
    "- train a logistic regression model on just the training set,\n",
    "- use the model to predict labels for the test set,\n",
    "- evaluate the accuracy of the predicted labels for the test set.\n",
    "\n",
    "Recall that accuracy helps us answer the question:\n",
    "\n",
    "- **What fraction of the predictions were correct (actual label matched predicted label)?**\n",
    "\n",
    "Prediction accuracy boils down to the number of labels that were correctly predicted divided by the total number of observations:\n",
    "\n",
    "$$Accuracy = \\dfrac{\\text{# of Correctly Predicted}}{\\text{# of Observations}}$$\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "- Train a logistic regression model using the gpa column from the train Dataframe.\n",
    "- Use the LogisticRegression method predict to return the predicted labels for the gpa column from the test Dataframe. Assign the resulting list of labels to the predicted_label column in the test Dataframe.\n",
    "- Calculate the accuracy of the predictions by dividing the number of rows where actual_label matches predicted_label by the total number of rows in the test set.\n",
    "- Assign the accuracy value to accuracy and display it using the print function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.635658914729\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(train[['gpa']], train['actual_label'])\n",
    "labels = clf.predict(test[['gpa']])\n",
    "test['predicted_label'] = labels\n",
    "\n",
    "accuracy = sum(test.predicted_label == test.actual_label) / test.shape[0]\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Sensitivity And Specificity\n",
    "\n",
    "Looks like the prediction accuracy is about **63.6%**, which isn't too far off from the accuracy value we computed in the previous mission of **64.6%**. If the model performed significantly worse on new data, this means that it's overfitting. If the prediction accuracy was much lower, say **40%** instead of **69%**, we would reconsider using logistic regression.\n",
    "\n",
    "When we evaluated the model on the training data in the previous mission, we achieved a sensitivity value of **12.7%** and a specificity value of **96.3%**. Let's calculate these measures for the test set and compare. Here's a quick refresher of sensitivity and specificity:\n",
    "\n",
    "- Sensitivity helps us answer the question:\n",
    "    - **How effective is this model at identifying positive outcomes?**\n",
    "    - Of all of the students that should have been admitted (True Positives + False Negatives), how many did the model correctly admit (True Positives)?\n",
    "\n",
    "- Specificity helps us answer the question:\n",
    "    - **How effective is this model at identifying negative outcomes?**\n",
    "    - Of all of the applicants who should have been rejected (False Positives + True Negatives), what proportion were correctly rejected (just True Negatives).\n",
    "\n",
    "Now it's your turn! Calculate the specificity and sensitivity values for the predictions on the test set. To encourage you to avoid relying on the formulas for these measures, we've hidden the exact formula in the **Hint** and prefer that you work backwards from the goals of these measures instead.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "- Calculate the sensitivity value for the predictions on the test set and assign to sensitivity.\n",
    "- Calculate the specificity value for the predictions on the test set and assign to specificity.\n",
    "- Display both values using the print function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sensitivity (TPR):  0.08333333333333333 \n",
      "specificity (TNR):  0.9629629629629629\n"
     ]
    }
   ],
   "source": [
    "true_positive = len(test[(test.actual_label == 1) & (test.predicted_label == 1)])\n",
    "true_negative = len(test[(test.actual_label == 0) & (test.predicted_label == 0)])\n",
    "false_positive = len(test[(test.actual_label == 0) & (test.predicted_label == 1)])\n",
    "false_negative = len(test[(test.actual_label == 1) & (test.predicted_label == 0)])\n",
    "\n",
    "sensitivity = true_positive / (true_positive + false_negative)\n",
    "specificity = true_negative / (true_negative + false_positive)\n",
    "\n",
    "print(\"sensitivity (TPR): \", sensitivity, \"\\nspecificity (TNR): \", specificity)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: False Positive Rate\n",
    "\n",
    "It turns out that our test set achieved a sensitivity value of **8.3**, compared to a sensitivity value of **12.7%** from the previous mission, and a specificity value of **96.3%**, which matches the specificity value of **96.3%** from the previous mission. We have a little more evidence now that our logistic regression model is able to generalize to new data.\n",
    "\n",
    "So far, we've been using the LogisticRegression method `predict` to generate predictions for labels. For each observation, scikit-learn uses the logit function, with the optimal parameter value for the data the model was trained on, to return a probabillity value. If the probability value is larger than **50%**, the predicted label is `1` and if it's less than **50%**, the predictd label is `0`. For most problems, however, **50%** is not the optimal discrimination threshold. We need a way to vary the threshold and compute the measures at each threshold. Then, depending on the measure we want to optimize, we can find the appropriate threshold to use for predictions.\n",
    "\n",
    "The 2 common measures that are computed for each discrimination threshold are the **False Positive Rate (or fall-out)** and the **True Positive Rate (or sensitivity)**. While we've explored the latter measure, we haven't discussed fall-out:\n",
    "\n",
    "- Fall-out or False Positive Rate - The proportion of applicants who should have been rejected (actual_label equals 0) but were instead admitted (predicted_label equals 1):\n",
    "\n",
    "$$FPR=\\dfrac{\\text{False Positives}}{\\text{False Positives} + \\text{True Negatives}}$$\n",
    "\n",
    "These 2 rates describe how well the model accepts the right students and how poorly it rejects the wrong one:\n",
    "\n",
    "- True Positive Rate: The proportion of students that were admitted that should have been admitted.\n",
    "- False Positive Rate: The proportion of students that were accepted that should have been rejected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: ROC Curve\n",
    "\n",
    "We can vary the discrimination threshold and calculate the TPR and FPR for each value. This is called an **ROC curve**, which stands for **receiver operating characteristic curve**, and it allows us to understand a classification model's performance as the discrimination threshold is varied. To calculate the TPR and FPR values at each discrimination threshold, we can use the [scikit-learn roc_curve function](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html). This function will calculate the false positive rate and true positive rate for varying discrimination thresholds until both reach 0%.\n",
    "\n",
    "This function takes 2 required parameters:\n",
    "\n",
    "- `y_true`: list of the true labels for the observations,\n",
    "- `y_score`: list of the model's probability scores for those observations.\n",
    "\n",
    "As the example code in the [documentation](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) suggests, the `roc_curve` function returns 3 values which you can assign all at once:\n",
    "\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(labels, probabilities)\n",
    "\n",
    "You'll notice that the returned thresholds won't usually range from 0.0 to 1.0 and will instead constrains the result set to the minimum range where FPR and TPR range from 0.0 to 1.0. Once we have the FPR and TPR for each relevant threshold, we can plot the ROC curve using the Matplotlib plot function.\n",
    "\n",
    "#### Instructions:\n",
    "- Import the relevant scikit-learn package you need to calculate the ROC curve.\n",
    "- Use the model to return predicted probabilities for the test set.\n",
    "- Use the roc_curve function to return the FPR and TPR values for different thresholds.\n",
    "- Create and display a line plot with:\n",
    "    - the FPR values on the x-axis and\n",
    "    - the TPR values on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x12013cf60>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nktQwKUiSGiYFSVLDpCBJapgUJEmN/w/mJjcWl5/WWgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x120067278>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "probabilities = clf.predict_proba(test[['gpa']])\n",
    "\n",
    "from sklearn.metrics import roc_curve\n",
    "fpr, tpr, thresholds = roc_curve(test.actual_label, probabilities[:,1])\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(fpr, tpr)\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.title(\"ROC curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: Area Under The Curve\n",
    "\n",
    "When looking at an ROC curve, you want to keep an eye on how the 2 measures trade off and select an appropriate threshold based on your priorities. Given that the university accepts very few students and rejects most of them, it's probably more concerned with a higher True Positive Rate than a low False Positive Rate. The university benefits the most if it does a wonderful job admitting a select number of students that deserve to be admitted than focusing aggressively on accurately rejecting students.\n",
    "\n",
    "We can now go one step further and determine the **area under the curve** or **AUC** for short. The AUC describes the probability that the classifier will rank a random positive observation higher than a random negative observation. Since randomly guessing converges to a probability of `0.5`, the higher the AUC the more accurate the model seems to be.\n",
    "\n",
    "To calculate the AUC, we can use the [scikit-learn function roc_auc_score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html), which takes the same parameters as the roc_curve function and returns a single float value corresponding to the AUC.\n",
    "\n",
    "#### Instructions:\n",
    "- Calculate the AUC score for our model on the training set and assign to auc_score.\n",
    "- Use the print function to display auc_score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.57793209876543217"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "# the probability that a classifier will rank a randomly chosen positive instance higher \n",
    "# than a randomly chosen negative one (assuming 'positive' ranks higher than 'negative').\n",
    "\n",
    "auc_score = roc_auc_score(test.actual_label, probabilities[:,1])\n",
    "auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8: Next Steps\n",
    "\n",
    "With an AUC score of about **57.8%**, our model does a little bit better than **50%**, which would correspond to randomly guessing, but not as high as the university may like. This could imply that using just one feature in our model, GPA, to predict admissions isn't enough. All of the measures and scores we've learned about are different ways of thinking about accuracy and the important takeaway is that no single measure will tell us if we want to use a specific model or not. Understanding how individual scores are calculated and what they focus on help you converge onto a clearer picture. It's always important to understand what measures are the most important for the problem at hand.\n",
    "\n",
    "In the next mission, we'll switch gears and learn how we can use machine learning on problems that don't involve predicting a label. This type of machine learning is called **unsupervised machine learning** and we'll focus on a technique called clustering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
