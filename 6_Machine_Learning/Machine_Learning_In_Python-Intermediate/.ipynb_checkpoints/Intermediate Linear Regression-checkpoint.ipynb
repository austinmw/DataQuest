{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intermediate Linear Regression\n",
    "### Learn how to use linear regression to estimate the speed at which the Leaning Tower of Pisa was moving.\n",
    "\n",
    "##### Contents:\n",
    "- Pisa data\n",
    "- statsmodels.api as sm\n",
    "    - sm.add_constant(X) (1's const column)\n",
    "    - linear = sm.OLS(y, X)\n",
    "    - linearfit = linear.fit()\n",
    "    - linearfit.summary()\n",
    "    - yhat = linearfit.predict(X)\n",
    "    - residuals = yhat - y\n",
    "    - plt.hist(residuals)\n",
    "- Sum of Square Error (SSE) also known as Residual Sum of Squares (RSS) or Sum of Squared Residuals (SSR)\n",
    "- [Total Sum of Squares (TSS)](https://en.wikipedia.org/wiki/Total_sum_of_squares)\n",
    "- Regression Sum of Squares (RSS) [also known as explained sum of squares (ESS)](https://en.wikipedia.org/wiki/Explained_sum_of_squares) (**NOT** Residual Sum of Squares)\n",
    "    - TSS = RSS{ESS} + SSE\n",
    "        - Wiki: total sum of squares = explained sum of squares + residual sum of squares.\n",
    "    - MSE = Residual Sum of Squares / n\n",
    "- Coefficient of Determination (R-Squared)\n",
    "    - R-Squared = 1 - SSE/TSS = RSS/TSS\n",
    "    - the percentage of variation in the target variable explained by our model\n",
    "- linearfit.params\n",
    "- variance of coefficients\n",
    "\n",
    "\n",
    "**Note:** $\\hat y$ = predicted values, $\\bar y$ = mean value of response (Doesn't actually need the $i$ subscript)\n",
    "\n",
    "## 1: Introduction To The Data\n",
    "\n",
    "The Leaning Tower of Pisa is one of the largest tourist attractions in Italy. For hundreds of years this tower slowly leaned to one side, eventually reaching 5.5 degrees off axis, nearly 3 meters horizontal at it's peak. Yearly data was recorded from 1975 to 1987 measuring the tower's lean. In 1988 restoration began on the tower to stop more leaning in the future. The data is provided in the dataframe `pisa`. The column `lean` represents the number of meters the tower is off axis at the respective year. In this mission we will try to estimate the leaning rate using a linear regression and interpret its coefficient and statistics.\n",
    "\n",
    "<img src=\"https://dq-content.s3.amazonaws.com/Mr5AVef.png\">\n",
    "\n",
    "#### Instructions:\n",
    "- Create a scatter plot with `year` on the x-axis and `lean` on the y-axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lean</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.9642</td>\n",
       "      <td>1975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.9644</td>\n",
       "      <td>1976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.9656</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.9667</td>\n",
       "      <td>1978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.9673</td>\n",
       "      <td>1979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.9688</td>\n",
       "      <td>1980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2.9696</td>\n",
       "      <td>1981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2.9698</td>\n",
       "      <td>1982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2.9713</td>\n",
       "      <td>1983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.9717</td>\n",
       "      <td>1984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2.9725</td>\n",
       "      <td>1985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2.9742</td>\n",
       "      <td>1986</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2.9757</td>\n",
       "      <td>1987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lean  year\n",
       "0   2.9642  1975\n",
       "1   2.9644  1976\n",
       "2   2.9656  1977\n",
       "3   2.9667  1978\n",
       "4   2.9673  1979\n",
       "5   2.9688  1980\n",
       "6   2.9696  1981\n",
       "7   2.9698  1982\n",
       "8   2.9713  1983\n",
       "9   2.9717  1984\n",
       "10  2.9725  1985\n",
       "11  2.9742  1986\n",
       "12  2.9757  1987"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pisa = pandas.DataFrame({\"year\": range(1975, 1988), \n",
    "                         \"lean\": [2.9642, 2.9644, 2.9656, 2.9667, 2.9673, 2.9688, 2.9696, \n",
    "                                  2.9698, 2.9713, 2.9717, 2.9725, 2.9742, 2.9757]})\n",
    "pisa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x11ae8f470>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nBAD/H97elY2TcRa2AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10ee10ba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(pisa.year, pisa.lean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2: Fit The Linear Model\n",
    "\n",
    "From the scatter plot you just made, we visually see that a linear regression looks to be a good fit for the data. In this mission we will learn how to understand key statistical concepts of the linear regression model.\n",
    "\n",
    "Statsmodels is a library which allows for rigorous statistical analysis in python. For linear models, statsmodels provides ample statistical measures for proper evaluation. The class sm.OLS is used to fit linear models, standing for ordinary least squares. After the initialization of our model we fit data to it using the .fit() method that estimates the coefficients of the linear model. OLS() does not automatically add an intercept to our model. We can add a column of 1's to add another coefficient to our model and since the coefficient is multiplied by 1 we are given an intercept.\n",
    "\n",
    "#### Instructions: \n",
    "\n",
    "- linearfit contains the fitted linear model to our data. Print the summary of the model by using the method .summary().\n",
    "    - Don't worry about understanding all of the summary, we will cover many of the statistics in the following screens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/austin/anaconda/lib/python3.6/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n",
      "/Users/austin/anaconda/lib/python3.6/site-packages/scipy/stats/stats.py:1334: UserWarning: kurtosistest only valid for n>=20 ... continuing anyway, n=13\n",
      "  \"anyway, n=%i\" % int(n))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>          <td>lean</td>       <th>  R-squared:         </th> <td>   0.988</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.987</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   904.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Thu, 13 Jul 2017</td> <th>  Prob (F-statistic):</th> <td>6.50e-12</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>14:59:39</td>     <th>  Log-Likelihood:    </th> <td>  83.777</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>    13</td>      <th>  AIC:               </th> <td>  -163.6</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>    11</td>      <th>  BIC:               </th> <td>  -162.4</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "    <td></td>       <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th> <td>    1.1233</td> <td>    0.061</td> <td>   18.297</td> <td> 0.000</td> <td>    0.988</td> <td>    1.258</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>year</th>  <td>    0.0009</td> <td>  3.1e-05</td> <td>   30.069</td> <td> 0.000</td> <td>    0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 0.310</td> <th>  Durbin-Watson:     </th> <td>   1.642</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.856</td> <th>  Jarque-Bera (JB):  </th> <td>   0.450</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td> 0.094</td> <th>  Prob(JB):          </th> <td>   0.799</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.108</td> <th>  Cond. No.          </th> <td>1.05e+06</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                   lean   R-squared:                       0.988\n",
       "Model:                            OLS   Adj. R-squared:                  0.987\n",
       "Method:                 Least Squares   F-statistic:                     904.1\n",
       "Date:                Thu, 13 Jul 2017   Prob (F-statistic):           6.50e-12\n",
       "Time:                        14:59:39   Log-Likelihood:                 83.777\n",
       "No. Observations:                  13   AIC:                            -163.6\n",
       "Df Residuals:                      11   BIC:                            -162.4\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          1.1233      0.061     18.297      0.000       0.988       1.258\n",
       "year           0.0009    3.1e-05     30.069      0.000       0.001       0.001\n",
       "==============================================================================\n",
       "Omnibus:                        0.310   Durbin-Watson:                   1.642\n",
       "Prob(Omnibus):                  0.856   Jarque-Bera (JB):                0.450\n",
       "Skew:                           0.094   Prob(JB):                        0.799\n",
       "Kurtosis:                       2.108   Cond. No.                     1.05e+06\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 1.05e+06. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "y = pisa.lean # target\n",
    "X = pisa.year  # features\n",
    "X = sm.add_constant(X)  # add a column of 1's as the constant term\n",
    "\n",
    "# OLS -- Ordinary Least Squares Fit\n",
    "linear = sm.OLS(y, X)\n",
    "# fit model\n",
    "linearfit = linear.fit()\n",
    "\n",
    "linearfit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3: Define A Basic Linear Model\n",
    "\n",
    "We see that the printed summary contains a lot of information about our model. To understand these statistical measures we must start with a formal definition of a basic linear regression model. Mathematically, a basic linear regression model is defined as $y_i = \\beta_0 + \\beta_1 x_i + e_i$ where $e_i \\sim N(0,\\sigma^2)$ is the error term for each observation $i$ where $\\beta_0$ is the intercept and $\\beta_1$ is the slope. The residual for the prediction of observation $i$ is $e_i = \\hat{y_i} - y_i$ where $\\hat{y_i}$ is the prediction. $N(0, \\sigma^2)$ is a normal distribution with mean $0$ and a variance $\\sigma^2$ This means that the model assumes that the errors, known as residuals, between our prediction and observed values are normally distributed and that the average error is 0. Estimated coefficients, those which are modeled, will be referred to as $\\hat{\\beta_i}$ while $\\beta_i$ is the true coefficient. In the end, \\hat{y_i} = \\hat{\\beta_0} + \\hat{\\beta_1} x_i  is the model we will estimate.\n",
    "\n",
    "#### Instructions: \n",
    "\n",
    "- Using `linearfit` with data `X` and `y` predict the residuals.\n",
    "- Residuals are computed by subtracting the observed values from the predicted values.\n",
    "- Assign the residuals to variable `residuals`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0     2.963778\n",
      "1     2.964710\n",
      "2     2.965642\n",
      "3     2.966574\n",
      "4     2.967505\n",
      "5     2.968437\n",
      "6     2.969369\n",
      "7     2.970301\n",
      "8     2.971233\n",
      "9     2.972165\n",
      "10    2.973097\n",
      "11    2.974029\n",
      "12    2.974960\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Our predicted values of y\n",
    "yhat = linearfit.predict(X)\n",
    "print(yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "residuals = yhat - y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4: Histogram Of Residuals\n",
    "\n",
    "We've used histograms in the past to visualize the distribution of our data. By creating a histogram of our residuals we can visually accept or reject the assumption of normality of the errors. If the histogram of residuals looks similar to a bell curve then we will accept the assumption of normality. There are more rigorous statistical tests to test for normality which we will cover in future lessons.\n",
    "\n",
    "#### Instructions:\n",
    "- Create a histogram with 5 bins of the residuals using matplotlib's hist() function.\n",
    "    - The bins parameter allows us to specify the number of bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11ae48da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(residuals, bins=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5: Interpretation Of Histogram\n",
    "\n",
    "Our dataset only has 13 observations in it making it difficult to interpret such a plot. Though the plot looks somewhat normal the largest bin only has 4 observations. In this case we cannot reject the assumption that the residuals are normal. Let's move forward with this linear model and look at measures of statistical fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6: Sum Of Squares\n",
    "\n",
    "Many of the statistical measures used to evaluate linear regression models rely on three sum of squares measures. The three measures include Sum of Square Error (SSE), Regression Sum of Squares (RSS), and Total Sum of Squares (TSS). In aggregate each of these measures explain the variance of the entire model. We define the measures as the following:\n",
    "\n",
    "$SSE = \\sum_{i=1}^n (y_i - \\hat{y_i})^2 = \\sum_{i=1}^n e_i^2$. We see that SSE is the sum of all residuals giving us a measure between the model's prediction and the observed values.\n",
    "\n",
    "$RSS = \\sum_{i=1}^n (\\bar{y_i} - \\hat{y_i})^2 \\text{ where }\\bar{y_i} = \\dfrac{1}{n}\\sum_{i=1}^n y_i$. RSS measures the amount of explained variance which our model accounts for. For instance, if we were to predict every observation as the mean of the observed values then our model would be useless and RSS would be very low. A large RSS and small SSE can be an indicator of a strong model.\n",
    "\n",
    "$TSS = \\sum_{i=1}^n (y_i - \\bar{y_i})^2$. TSS measures the total amount of variation within the data.\n",
    "\n",
    "With some algebra we can show that $TSS = RSS +  SSE$. Intuitively this makes sense, the total amount of variance in the data is captured by the variance explained by the model plus the variance missed by the model.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "- Compute the RSS and TSS for our model, linearfit, using the formulas above.\n",
    "    - Assign the RSS to variable RSS and the TSS to variable TSS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9228571428566404e-06 0.00015804483516481858 0.00015996769230767523\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sum of Squared Error (also known as Residual Sum of Squares, RSS)\n",
    "# sum the (predicted - observed) squared\n",
    "SSE = np.sum((y.values-yhat)**2)\n",
    "\n",
    "# Regression Sum of Squares (also known as Explained Sum of Squares, ESS)\n",
    "# sum the (mean - predicted) squared\n",
    "RSS = np.sum((y.mean() - yhat)**2)\n",
    "\n",
    "# Total Sum of Squares \n",
    "# sum of the squared differences of each observation from the overall mean\n",
    "TSS = SSE + RSS\n",
    "\n",
    "print(SSE, RSS, TSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7: R-Squared\n",
    "\n",
    "The **coefficient of determination**, also known as R-Squared, is a great measure of linear dependence. It is a single number which tells us what the percentage of variation in the target variable is explained by our model.\n",
    "\n",
    "$$ R^2 = 1 - \\dfrac{SSE}{TSS} = \\dfrac{RSS}{TSS}$$\n",
    "\n",
    "Intuitively we know that a low SSE and high RSS indicates a good fit. This single measure tells us what percentage of the total variation of the data our model is accounting for. Correspondingly, the $R^2$ exists between 0 and 1.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "- Compute the R-Squared for our model, linearfit. Assign the R-squared to variable R2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.987979715684\n"
     ]
    }
   ],
   "source": [
    "SSE = np.sum((y.values-yhat)**2)\n",
    "ybar = np.mean(y.values)\n",
    "RSS = np.sum((ybar-yhat)**2)\n",
    "TSS = np.sum((y.values-ybar)**2)\n",
    "\n",
    "R2 = RSS / TSS\n",
    "print(R2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8: Interpretation Of R-Squared\n",
    "\n",
    "We see that the R-Squared value is very high for our linear model, 0.988, accounting for 98.8% of the variation within the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9: Coefficients Of The Linear Model\n",
    "\n",
    "The ability to understand and interpret coefficients is a huge advantage of linear models over some more complex ones. Each $\\beta_i$ in a linear model $f(x) = \\beta_0 + \\beta_1 x$ is a coefficient. Fortunately there are methods to find the confidence of the estimated coefficients.\n",
    "\n",
    "Below we see the summary of our model. There are many statistics here including R-squared, the number of observations, and others. In the second section there are coefficients with corresponding statistics. The row `year` corresponds to the independent variable x while `lean` is the target variable. The variable `const` represents the model's intercept.\n",
    "\n",
    "First we look at the coefficient itself. The coefficient measures how much the dependent variable will change with a unit increase in the independent variable. For instance, we see that the coefficient for year is 0.0009. This means that on average the tower of Pisa will lean 0.0009 meters per year.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "- Assuming no external forces on the tower, how many meters will the tower of Pisa lean in `15` years?\n",
    "    - Assign the number of meters moved to variable `delta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " const    1.123338\n",
      "year     0.000932\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Print the models summary\n",
    "#print(linearfit.summary())\n",
    "\n",
    "#The models parameters\n",
    "print(\"\\n\",linearfit.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "delta = linearfit.params[\"year\"] * 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10: [Variance Of Coefficients](https://en.wikipedia.org/wiki/Regression_analysis)\n",
    "\n",
    "The variance of each of the coefficients is an important and powerful measure. In our example the coefficient of `year` represents the number of meters the tower will lean each year. The variance of this coefficient would then give us an interval of the expected movement for each year.\n",
    "\n",
    "In the summary output, next to each coefficient, you see a column with standard errors. The standard error is the square root of the estimated variance. The estimated variance for a single variable linear model is defined as:\n",
    "\n",
    "$$s^2(\\hat{\\beta_1}) = \\dfrac{\\sum_{i=1}^n (y_i - \\hat{y_i})^2}{(n-2) \\sum_{i=1}^n (x_i - \\bar{x})^2} = \\dfrac{SSE}{(n-2) \\sum_{i=1}^n (x_i - \\bar{x})^2}$$\n",
    "\n",
    "This formulation can be shown by taking the variance of our estimated $\\hat{\\beta_1}$ but we will not cover that here. Analyzing the formula term by term we see that the numerator, SSE, represents the error within the model. A small error in the model will then decrease the coefficient's variance. The denominator term, $\\sum_{i=1}^n (x_i - \\bar{x})^2$, measures the amount of variance within x. A large variance within the independent variable decreases the coefficient's variance. The entire value is divided by (n-2) to normalize over the SSE terms while accounting for 2 degrees of freedom in the model.\n",
    "\n",
    "Using this variance we will be able to compute t-statistics and confidence intervals regarding this $\\beta_1$. We will get to these in the following screens.\n",
    "\n",
    "**Note:** The denominator is the sample size reduced by the number of model parameters estimated from the same data, $(n-p)$, for $p$ regressors or $(n-p-1$ if an intercept is used. In this case, $p=1$ so the denominator is $n-2$.\n",
    "\n",
    "#### Instructions:\n",
    "- Compute $s^2({\\hat{\\beta_1}})$ for `linearfit`.\n",
    "    - Assign this variance to variable `s2b1`.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9.604681033249953e-10\n"
     ]
    }
   ],
   "source": [
    "xvar = np.sum((pisa.year - pisa.year.mean())**2)\n",
    "s2b1 = SSE / ((len(y) - 2) * xvar)\n",
    "\n",
    "print(s2b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11: T-Distribution\n",
    "\n",
    "Statistical tests can be done to show that the lean of the tower is dependent on the year. A common test of statistical significance is the student t-test. The student t-test relies on the t-distribution, which is very similar to the normal distribution, following the bell curve but with a lower peak.\n",
    "\n",
    "The t-distribution accounts for the number of observations in our sample set while the normal distribution assumes we have the entire population. In general, the smaller the sample we have the less confidence we have in our estimates. The t-distribution takes this into account by increasing the variance relative to the number of observations. You will see that as the number of observations increases the t-distribution approaches the normal distribution.\n",
    "\n",
    "The density functions of the t-distributions are used in significance testing. The probability density function (pdf) models the relative likelihood of a continuous random variable. The cumulative density function (cdf) models the probability of a random variable being less than or equal to a point. The degrees of freedom (df) accounts for the number of observations in the sample. In general the degrees of freedom will be equal to the number of observations minus 2. Say we had a sample size with just 2 observations, we could fit a line through them perfectly and no error in the model. To account for this we subtract 2 from the number of observations to compute the degrees of freedom.\n",
    "\n",
    "Scipy has functions in the library `scipy.stats.t` which can be used to compute the pdf and cdf of the t-distribution for any number of degrees of freedom. `scipy.stats.t.pdf(x,df)` is used to estimate the pdf at variable x with df degrees of freedom.\n",
    "\n",
    "#### Instructions:\n",
    "- Make a plot comparing 2 pdfs of the t-distribution with different degrees of freedom.\n",
    "- With the `x` variable given, compute the pdf with 3 degrees of freedom and assign it to variable `tdist3`.\n",
    "- Then compute a similar pdf with 30 degrees of freedom and assign it to variable `tdist30`.\n",
    "- On a single plot, plot `x` on the x-axis for both `tdist3` and `tdist30` on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02297204  0.02441481  0.02596406  0.02762847  0.0294174   0.031341\n",
      "  0.03341025  0.03563701  0.03803403  0.04061509  0.04339497  0.04638952\n",
      "  0.04961567  0.05309149  0.05683617  0.06086996  0.0652142   0.06989116\n",
      "  0.07492395  0.08033633  0.08615245  0.09239652  0.0990924   0.10626304\n",
      "  0.11392986  0.12211193  0.13082504  0.14008063  0.14988449  0.16023537\n",
      "  0.17112343  0.18252859  0.1944188   0.20674834  0.21945618  0.23246464\n",
      "  0.2456783   0.2589835   0.27224841  0.28532401  0.29804594  0.31023748\n",
      "  0.32171351  0.33228555  0.34176766  0.34998293  0.35677032  0.36199128\n",
      "  0.36553585  0.36732769  0.36732769  0.36553585  0.36199128  0.35677032\n",
      "  0.34998293  0.34176766  0.33228555  0.32171351  0.31023748  0.29804594\n",
      "  0.28532401  0.27224841  0.2589835   0.2456783   0.23246464  0.21945618\n",
      "  0.20674834  0.1944188   0.18252859  0.17112343  0.16023537  0.14988449\n",
      "  0.14008063  0.13082504  0.12211193  0.11392986  0.10626304  0.0990924\n",
      "  0.09239652  0.08615245  0.08033633  0.07492395  0.06989116  0.0652142\n",
      "  0.06086996  0.05683617  0.05309149  0.04961567  0.04638952  0.04339497\n",
      "  0.04061509  0.03803403  0.03563701  0.03341025  0.031341    0.0294174\n",
      "  0.02762847  0.02596406  0.02441481  0.02297204]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import t\n",
    "\n",
    "# 100 values between -3 and 3\n",
    "x = np.linspace(-3,3,100)\n",
    "\n",
    "# Compute the pdf with 3 degrees of freedom\n",
    "print(t.pdf(x=x, df=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
